{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LorenzoPolli/market-basket-analysis/blob/main/A_Priori_%26_SON_Market_basket_analysis_(eng_newspapers).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzPd2cVi4h0p"
      },
      "source": [
        "# Market-basket analysis  ðŸ§º\n",
        "\n",
        "The task is to implement a system finding frequent itemsets (aka market-basket analysis), analyzing the Â«Old NewspapersÂ» dataset published on Kaggle and released under the public domain license (CC0)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Project authors: Mathias Cardarello Fierro & Lorenzo Polli\n",
        "\n",
        "Algorithms for Massive Data\n",
        "\n",
        "\n",
        "*UniversitÃ  degli Studi di Milano*\n",
        "\n",
        "\n",
        "15-Dec-2022\n"
      ],
      "metadata": {
        "id": "G7a8BDBDDyRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A-Priori & SON Algorithms**"
      ],
      "metadata": {
        "id": "qcTc30vECquC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTzg-Ca1_v6x"
      },
      "source": [
        "### **1. Setup and data import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "BjsA1sbWTf2e"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Download the dataset containing old newspapers\n",
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"mathiascardarello\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"89f16dcdf267d017756e3a2e5cece19a\"\n",
        "!pip install kaggle --upgrade\n",
        "!kaggle datasets download alvations/old-newspapers --unzip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Java 8 openjdk\n",
        "\n",
        "%%capture\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "!pip install py4j"
      ],
      "metadata": {
        "id": "EXLVNXPH6Dt8"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl0osm4q9rP3",
        "outputId": "afe77ffa-2183-4612-dbe7-d780aab64251"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.17\" 2022-10-18\n",
            "OpenJDK Runtime Environment (build 11.0.17+8-post-Ubuntu-1ubuntu218.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.17+8-post-Ubuntu-1ubuntu218.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPU0IrIShQOW"
      },
      "source": [
        "#### **1.1 Setting up PySpark and Spark NLP**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pyspark and spark-nlp\n",
        "\n",
        "%%capture\n",
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ],
      "metadata": {
        "id": "a6h0fOXD90rz"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzR3wOaKn6Fh",
        "outputId": "2e5db7f8-dc66-412e-9cb0-90a391770f28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version 4.2.4\n",
            "Apache Spark version: 3.2.1\n",
            "CPU times: user 5.18 ms, sys: 89 Âµs, total: 5.27 ms\n",
            "Wall time: 36.4 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version\", sparknlp.version())\n",
        "print(\"Apache Spark version:\", spark.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "RZ8MxGvNo-7k",
        "outputId": "f4db8f50-8686-4c3f-b9c7-e3ca08c662a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f04e9132340>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://0504a412ece3:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Check Spark environment\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8aKsVAreVlH"
      },
      "source": [
        "#### **1.2. Import the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7gV2qM6rco4",
        "outputId": "b870305c-d52f-463e-eb31-11f1c67cc83b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------+----------+--------------------+\n",
            "|Language|      Source|      Date|                Text|\n",
            "+--------+------------+----------+--------------------+\n",
            "| English| latimes.com|2012/04/29|He wasn't home al...|\n",
            "| English|stltoday.com|2011/07/10|The St. Louis pla...|\n",
            "| English|   freep.com|2012/05/07|WSU's plans quick...|\n",
            "| English|      nj.com|2011/02/05|The Alaimo Group ...|\n",
            "| English|  sacbee.com|2011/10/02|And when it's oft...|\n",
            "+--------+------------+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "CPU times: user 303 ms, sys: 33.1 ms, total: 336 ms\n",
            "Wall time: 49.2 s\n"
          ]
        }
      ],
      "source": [
        "# Import the dataset and display only articles in English\n",
        "\n",
        "%%time\n",
        "df = spark.read.csv('old-newspaper.tsv', sep='\\t', header=True) \n",
        "df = df.filter(\"Language == 'English'\")\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "E1zwWG0qlP70"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XN2N_UP4YjN",
        "outputId": "17bd699c-c973-413c-c86f-7ed036db1d30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+----+----+\n",
            "|Language|Source|Date|Text|\n",
            "+--------+------+----+----+\n",
            "|       0|     0|   0|   0|\n",
            "+--------+------+----+----+\n",
            "\n",
            "CPU times: user 498 ms, sys: 64.3 ms, total: 562 ms\n",
            "Wall time: 1min 27s\n"
          ]
        }
      ],
      "source": [
        "# Check whether there are NULL values or not\n",
        "\n",
        "%%time\n",
        "df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfDgujX_eN_A"
      },
      "source": [
        "### **2. Pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to prepare our dataset to implement the market-basket analysis, considering values of the \"Text\" attribute as baskets and the words as items. Therefore, a dataframe is created in which each row represents a basket with their comma-separated items.\n",
        "\n",
        "Furthermore, we apply text mining techniques to filter out stop words to get a powerful analysis of the text."
      ],
      "metadata": {
        "id": "NfH1GS0cf7Uo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "AkTW64M4ldrG"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "import pyspark.sql.types as T\n",
        "from typing import List\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.pretrained import PretrainedPipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpIZhnA1us_J",
        "outputId": "b30c305e-2abb-426f-e38b-bca1ce4297f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stopwords_en download started this may take some time.\n",
            "Approximate size to download 2.9 KB\n",
            "[OK!]\n",
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "# Initialize the annotators\n",
        "\n",
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"Text\")\\\n",
        "    .setOutputCol(\"document\")\\\n",
        "    .setCleanupMode(\"shrink\") # remove new lines and tabs, plus merging multiple spaces and blank lines to a single space\n",
        "\n",
        "sentence = SentenceDetector()\\\n",
        "    .setInputCols(['document'])\\\n",
        "    .setOutputCol('sentence')\n",
        "\n",
        "token = Tokenizer()\\\n",
        "    .setInputCols(['sentence'])\\\n",
        "    .setOutputCol('token')\n",
        "\n",
        "normalizer = Normalizer()\\\n",
        "    .setInputCols([\"token\"])\\\n",
        "    .setOutputCol(\"normalized\")\\\n",
        "    .setLowercase(True)\\\n",
        "    .setCleanupPatterns([\"\"\"[^\\w\\d\\s]\"\"\"]) # remove punctuations (keep alphanumeric characterss)\n",
        "\n",
        "stop_words = StopWordsCleaner.pretrained('stopwords_en', 'en')\\\n",
        "    .setInputCols([\"normalized\"])\\\n",
        "    .setOutputCol(\"cleanTokens\")\\\n",
        "    .setCaseSensitive(False)\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained()\\\n",
        "    .setInputCols([\"cleanTokens\"])\\\n",
        "    .setOutputCol(\"lemma\")\n",
        "\n",
        "prediction_pipeline = Pipeline(stages = [document, sentence, token, normalizer, stop_words, lemmatizer])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "GF5kjlwSvBuj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0a9f9ca-a119-4692-cb51-0fd962422299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 132 ms, sys: 23.3 ms, total: 155 ms\n",
            "Wall time: 1.05 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "clean_df = prediction_pipeline.fit(df).transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "ePOvOAw2Gn03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56990b6-2239-4e3c-bc90-5851181dcf46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.15 ms, sys: 914 Âµs, total: 6.06 ms\n",
            "Wall time: 76 ms\n"
          ]
        }
      ],
      "source": [
        "# Add a column where each row corresponds to a different basket of items \n",
        "\n",
        "%%time\n",
        "clean_df = clean_df.withColumn(\"Basket\", clean_df.lemma.result) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eWYkD_J75wJ"
      },
      "source": [
        "Note that in the last row, item \"love\" is repeated twice, and this is not good for further algorithms application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "urP8MzWUGNOK"
      },
      "outputs": [],
      "source": [
        "# Remove duplicates inside baskets\n",
        "remove_duplicates = F.udf(lambda x: list(set(x)))\n",
        "clean_df = clean_df.withColumn(\"BasketNoDup\", remove_duplicates(F.col(\"Basket\")))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove strings \"[\" and \"]\" from baskets -> otherwise, some items are not easily identified \n",
        "clean_df = clean_df.withColumn(\"BasketNoBrackets\", \n",
        "                F.regexp_replace(F.regexp_replace(F.regexp_replace(\"BasketNoDup\", \"\\\\]\\\\[\", \"\"), \"\\\\[\", \"\"), \"\\\\]\", \"\"))"
      ],
      "metadata": {
        "id": "EMVh9mBT1ZXr"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add an index column to optimize computation time\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "clean_df = clean_df.withColumn(\"Index\", monotonically_increasing_id()) "
      ],
      "metadata": {
        "id": "LhjNEDTskc2z"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show DataFrame after transformations\n",
        "clean_df.select(\"Index\",\"Basket\",\"BasketNoDup\",\"BasketNoBrackets\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cpqH4PJ2wJC",
        "outputId": "e9f05b8a-c765-4661-e446-8a6f444ddd18"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------+--------------------+--------------------+\n",
            "|      Index|              Basket|         BasketNoDup|    BasketNoBrackets|\n",
            "+-----------+--------------------+--------------------+--------------------+\n",
            "|85899345920|[wasnt, home, app...|[apparently, home...|apparently, home,...|\n",
            "|85899345921|[st, louis, plant...|[production, mass...|production, mass,...|\n",
            "|85899345922|[wsus, plan, quic...|[applaud, center,...|applaud, center, ...|\n",
            "|85899345923|[alaimo, group, m...|[employee, june, ...|employee, june, g...|\n",
            "|85899345924|[difficult, predi...|[merit, absolutel...|merit, absolutely...|\n",
            "|85899345925|[amount, scoff, y...|[year, nfl, round...|year, nfl, round,...|\n",
            "|85899345926|[14915, charlevoi...|[charlevoix, 1491...|charlevoix, 14915...|\n",
            "|85899345927|[long, line, fail...|[gop, lonegan, ci...|gop, lonegan, cit...|\n",
            "|85899345928|[time, report, su...|[employee, improv...|employee, improve...|\n",
            "|85899345929|[hit, hard, somep...|[rizzo, im, field...|rizzo, im, field,...|\n",
            "|85899345930|[mhta, president,...|[mhta, anderson, ...|mhta, anderson, f...|\n",
            "|85899345931|[absurdity, attem...|[attempt, bottle,...|attempt, bottle, ...|\n",
            "|85899345932|[gm, labor, relat...|[competitive, dea...|competitive, deal...|\n",
            "|85899345933|[wandry, matter, ...|[impose, drop, ta...|impose, drop, tax...|\n",
            "|85899345934|[cheap, hit, hard...|[6foot10, rise, s...|6foot10, rise, sq...|\n",
            "|85899345935|[andrades, child,...|[school, son, stu...|school, son, stud...|\n",
            "|85899345936|              [hair]|              [hair]|                hair|\n",
            "|85899345937|[bear, april, 15,...|[april, pozzuoli,...|april, pozzuoli, ...|\n",
            "|85899345938|[house, minority,...|[house, democrat,...|house, democrat, ...|\n",
            "|85899345939|[love, love, mone...|[seek, lie, human...|seek, lie, human,...|\n",
            "+-----------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new object that contains just text and baskets with no duplicated words\n",
        "\n",
        "clean_df.createOrReplaceTempView(\"df_view\")\n",
        "\n",
        "baskets = spark.sql(\"SELECT Index, SPLIT(BasketNoBrackets,', ') AS Basket FROM df_view\")"
      ],
      "metadata": {
        "id": "_uJljihpdCr-"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column \"Basket\" must be of the type \"Array of strings\"\n",
        "baskets.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifXYUgVh31hG",
        "outputId": "318b453e-659c-4d83-eb33-2a08e3f39a56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Index: long (nullable = false)\n",
            " |-- Basket: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain a sample to work with that is 0.5% the size of the original dataset\n",
        "sample = baskets.select(\"Index\",\"Basket\").sample(False, 0.005, 10)"
      ],
      "metadata": {
        "id": "o2kxWvw_gl2J"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBaXQfII7COM",
        "outputId": "99bb3854-b6d8-478d-9451-23f15743837e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Index      |Basket                                                                                                                                                                              |\n",
            "+-----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|85899345935|[school, son, student, mentore, child, andrades, opportunity, adult, play, role, intrigue, patrick, life, high, erin]                                                               |\n",
            "|85899346237|[fish, jeopardize, threaten, adversely, habitat, zidells, plan, conclude, wild, modify]                                                                                             |\n",
            "|85899346277|[shut, shoulder, freddy, week, baseman, sanchez, temporarily, throw, rehab, time]                                                                                                   |\n",
            "|85899346483|[development, face, ease, critic, leader, insurance, move, law, healthcare, state, statebased, resistance, key, administration, monday, feature, mount, obama, republican, exchange]|\n",
            "|85899346702|[mayberry, alfredo, walk, make, gonzalez, single, jr, throw, john, amezaga, inning, open]                                                                                           |\n",
            "+-----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxopvkmDvMlf"
      },
      "source": [
        "### **3. Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtmsomlG_Ty3",
        "outputId": "49bdb304-3972-42b8-af93-3cf0af1d7412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|total_rows|\n",
            "+----------+\n",
            "|   1010092|\n",
            "+----------+\n",
            "\n",
            "CPU times: user 387 ms, sys: 52.1 ms, total: 439 ms\n",
            "Wall time: 1min 7s\n"
          ]
        }
      ],
      "source": [
        "# Total number of articles\n",
        "\n",
        "%%time\n",
        "total_rows = spark.sql(\"\"\"SELECT COUNT(DISTINCT Text) AS total_rows \n",
        "                        FROM df_view\"\"\")\n",
        "total_rows.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of articles by source\n",
        "\n",
        "%%time\n",
        "top_sources = spark.sql(\"\"\"SELECT DISTINCT source AS source_name,\n",
        "                                  COUNT(DISTINCT Text) AS total_rows \n",
        "                        FROM df_view\n",
        "                        GROUP BY source_name\n",
        "                        ORDER BY total_rows DESC\n",
        "                        LIMIT 10\"\"\")\n",
        "top_sources.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxXc9cCnWOc4",
        "outputId": "4f682b91-a523-4b9d-b83e-66854a46836c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----------+\n",
            "|     source_name|total_rows|\n",
            "+----------------+----------+\n",
            "|   cleveland.com|    152716|\n",
            "|          nj.com|    125100|\n",
            "|    stltoday.com|    120632|\n",
            "|  oregonlive.com|    103494|\n",
            "|     latimes.com|     60637|\n",
            "|   azcentral.com|     42693|\n",
            "|      sfgate.com|     42121|\n",
            "|baltimoresun.com|     39994|\n",
            "|       freep.com|     36466|\n",
            "| startribune.com|     31820|\n",
            "+----------------+----------+\n",
            "\n",
            "CPU times: user 420 ms, sys: 55.2 ms, total: 475 ms\n",
            "Wall time: 1min 12s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of articles by year\n",
        "\n",
        "%%time\n",
        "by_year = spark.sql(\"\"\"SELECT DISTINCT substr(Date, 1, 4) AS year,\n",
        "                              COUNT(DISTINCT Text) AS total_rows \n",
        "                        FROM df_view\n",
        "                        GROUP BY year\n",
        "                        ORDER BY year\"\"\")\n",
        "by_year.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i61x4QSGW8ur",
        "outputId": "8c42b33d-512d-4057-f69a-19c5e8569f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+\n",
            "|year|total_rows|\n",
            "+----+----------+\n",
            "|2005|      3859|\n",
            "|2006|      7076|\n",
            "|2007|      8694|\n",
            "|2008|     17909|\n",
            "|2009|     42158|\n",
            "|2010|    122991|\n",
            "|2011|    263965|\n",
            "|2012|    543444|\n",
            "+----+----------+\n",
            "\n",
            "CPU times: user 456 ms, sys: 55 ms, total: 511 ms\n",
            "Wall time: 1min 13s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jddR0mW9vMTZ"
      },
      "source": [
        "### **4. Market-Basket Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.1 A-priori Algorithm**"
      ],
      "metadata": {
        "id": "P4yMXT3whyKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A-priori\n",
        "\n",
        "### FUNCTIONS ###\n",
        "# Definition of sum function\n",
        "def sum_items(x,y):\n",
        "    return x+y\n",
        "\n",
        "# Definition of filtering function: it checks if a set of items is a subset of a basket\n",
        "def filtering(basket_list, candidates):\n",
        "  for item in candidates:\n",
        "    if set(item).issubset(set(basket_list)):\n",
        "      return ((item, 1))"
      ],
      "metadata": {
        "id": "3S1w3HOXmmWD"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The Algorithm implementation "
      ],
      "metadata": {
        "id": "W7hrx81Hpez6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert column containing baskets into a Resilient Distributed Dataset (RDD)\n",
        "rdd_baskets = sample.select(\"Basket\").rdd"
      ],
      "metadata": {
        "id": "-0wxMkSeCuIk"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a single array from an array of arrays\n",
        "flat_list = rdd_baskets.flatMap(lambda xs: [x[0] for x in xs])"
      ],
      "metadata": {
        "id": "BH_blVQUuyhR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minSupport = 50\n",
        "\n",
        "singletons = flat_list.map(lambda item: (item, 1)) # create a set of (key, value) pairs \n",
        "singletons_sum = singletons.reduceByKey(sum_items) # count frequency of each key\n",
        "singletons_filtered = singletons_sum.filter(lambda item: item[1] >= minSupport) # filter out items that are below the threshold (not frequent)"
      ],
      "metadata": {
        "id": "72bbRe_NzXPz"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequent_items = singletons_filtered.map(lambda item: item[0]) # obtain a list of the frequent items"
      ],
      "metadata": {
        "id": "FbTrtKWH5kUR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import combinations module\n",
        "from itertools import combinations"
      ],
      "metadata": {
        "id": "pPUQyuJYqQ78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_list = list(combinations(frequent_items.toLocalIterator(),2)) # a list of all the possible frequent items combinations (pairs)"
      ],
      "metadata": {
        "id": "U5gihu0s6BUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlvScebpM_cH",
        "outputId": "46687fd5-32aa-4085-8339-d6b559c04ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('work', 'year'),\n",
              " ('work', 'start'),\n",
              " ('work', 'school'),\n",
              " ('year', 'start'),\n",
              " ('year', 'school'),\n",
              " ('start', 'school')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: the most frequent items are \"work\", \"year\", \"start\", \"school\"."
      ],
      "metadata": {
        "id": "VWWaxOhEqcmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the support table for the pairs of items by applying the filtering function previously created\n",
        "supp_table_pairs = basket_list.map(lambda x : filtering(x, pairs_list)).filter(lambda x: x is not None) # apply filtering function to check if a pair appears in the baskets"
      ],
      "metadata": {
        "id": "278EO9KiCa2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supp_table_pairs_sum = supp_table_pairs.reduceByKey(sum_items) # sum of values by item as key"
      ],
      "metadata": {
        "id": "Rh7tk8D_kHFy"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ALGORITHM IMPLEMENTATION ###\n",
        "def a_priori(rdd_baskets, minSupport):\n",
        "  '''\n",
        "  Apply the a-priori algorithm to singletons only to obtain a candidate itemset of frequent items\n",
        "  '''\n",
        "\n",
        "  # Create a single array from an array of arrays\n",
        "  flat_list = rdd_baskets.flatMap(lambda xs: [x[0] for x in xs])\n",
        "\n",
        "  singletons = flat_list.map(lambda item: (item, 1)).cache() # create a set of (key, value) pairs and cache the results\n",
        "  singletons_sum = singletons.reduceByKey(sum_items) # count frequency of each key\n",
        "  singletons_filtered = singletons_sum.filter(lambda item: item[1] >= minSupport) # filter out items that are below the threshold (not frequent)\n",
        "\n",
        "  candidate_itemset = singletons_filtered.map(lambda item: (item[0], 1)) # obtain the candidate itemset\n",
        "\n",
        "  # Consider pairs of items -> in this case, the output of the function should be modified\n",
        "\n",
        "  # frequent_items = singletons_filtered.map(lambda item: (item[0])) # retain only frequent items\n",
        "  # pairs_list = list(combinations(frequent_items.toLocalIterator(),2)) # a list of all the possible frequent items combinations (pairs)\n",
        "\n",
        "  # Create the support table for the pairs of items by applying the filtering function previously created\n",
        "  # supp_table_pairs = rdd_baskets.map(lambda x : filtering(x, pairs_list)).filter(lambda x: x is not None) # apply filtering function to check if a pair appears in the baskets\n",
        "\n",
        "  return (candidate_itemset)"
      ],
      "metadata": {
        "id": "MHJyvjTv8kdt"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.2 SON Algorithm**"
      ],
      "metadata": {
        "id": "o3Vzw23VouNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce number of partitions from default value (45) to 10.\n",
        "rdd_baskets = rdd_baskets.coalesce(10)"
      ],
      "metadata": {
        "id": "YXPjHcbtUL-b"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minSupport = 50\n",
        "numPartitions = rdd_baskets.getNumPartitions()\n",
        "adjSupport = minSupport/numPartitions\n",
        "adjSupport"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ6WkGSL275x",
        "outputId": "353f1203-5fba-4f17-ece5-f23ffd9998be"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.0"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The Algorithm implementation"
      ],
      "metadata": {
        "id": "qndpsKn0E5aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "candidates = spark.sparkContext.parallelize([]) # create an empty RDD to merge the results of a-priori applied on each chunk\n",
        "\n",
        "for i in range(0, numPartitions-1):\n",
        "  partition = spark.sparkContext.parallelize(rdd_baskets.glom().collect()[i]) # collect each partition by using glom method\n",
        "  candidate_chunk = a_priori(partition, adjSupport) # call the a-priori function and apply it to the partition  \n",
        "  candidates = candidates.union(candidate_chunk) # add local frequent itemset to candidates RDD"
      ],
      "metadata": {
        "id": "oBTD1hEbE_NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually select the minimum global support to filter out infrequent items\n",
        "minTotalSupport = 1\n",
        "\n",
        "# Apply reduce function to candidates and filter out not frequent items (false negatives)\n",
        "candidates_sum = candidates.reduceByKey(sum_items) # count frequency of each key\n",
        "candidates_filtered = candidates_sum.filter(lambda item: item[1] >= minTotalSupport) # filter out items that are below the threshold (not frequent)"
      ],
      "metadata": {
        "id": "PnrsfgJ-eBc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Results**"
      ],
      "metadata": {
        "id": "HTAH-UQ0LCaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SQLContext\n",
        "sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n",
        "\n",
        "# Display SON algorithm results\n",
        "result_columns = [\"Item\",\"Freq\"]\n",
        "results = candidates_filtered.toDF(result_columns)\n",
        "results.createOrReplaceTempView(\"results\")\n",
        "sqlContext.sql(\"SELECT * FROM results ORDER BY Freq DESC\").show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6M9tMqJgPHt",
        "outputId": "037349c3-ff44-41f9-9e59-6ca4322e5be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----+\n",
            "|Item    |Freq|\n",
            "+--------+----+\n",
            "|guard   |1   |\n",
            "|family  |1   |\n",
            "|score   |1   |\n",
            "|10      |1   |\n",
            "|seventh |1   |\n",
            "|head    |1   |\n",
            "|        |1   |\n",
            "|scott   |1   |\n",
            "|step    |1   |\n",
            "|healthy |1   |\n",
            "|athletic|1   |\n",
            "|official|1   |\n",
            "|put     |1   |\n",
            "|gross   |1   |\n",
            "|catholic|1   |\n",
            "|gop     |1   |\n",
            "|think   |1   |\n",
            "|stock   |1   |\n",
            "|federal |1   |\n",
            "|return  |1   |\n",
            "+--------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert RDD containing the most freq items to dataframe\n",
        "freq_columns = [\"Item\", \"Freq\"]\n",
        "freq_df = singletons_filtered.toDF(freq_columns)\n",
        "freq_df.createOrReplaceTempView(\"freq_df\")\n",
        "sqlContext.sql(\"SELECT * FROM freq_df ORDER BY Freq DESC\").show(5, truncate = False)"
      ],
      "metadata": {
        "id": "VyF2uQ6j8ydc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d88355eb-0690-43d1-a26a-7175fe3cff1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+\n",
            "|Item  |Freq|\n",
            "+------+----+\n",
            "|work  |107 |\n",
            "|year  |73  |\n",
            "|school|65  |\n",
            "|start |61  |\n",
            "+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve total support for a specific singleton\n",
        "work_support = sqlContext.sql(\"SELECT Item, Freq FROM freq_df WHERE Item=='work'\")\n",
        "work_support.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZUBz82jueu-",
        "outputId": "be2fa4ab-3a12-4db9-a6d6-61500ea3a155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+\n",
            "|Item|Freq|\n",
            "+----+----+\n",
            "|work| 107|\n",
            "+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: here the error \"unhashable type 'list'\" occurs. It refers back to the \"filtering\" function\n",
        "\n",
        "# Display frequent pairs and their support \n",
        "freq_pairs_columns = [\"Pair\", \"Freq\"]\n",
        "freq_pairs_df = supp_table_pairs_sum.toDF(freq_pairs_columns)\n",
        "freq_pairs_df.createOrReplaceTempView(\"freq_pairs_df\")\n",
        "sqlContext.sql(\"SELECT * FROM freq_pairs_df ORDER BY Freq DESC\").show(5, truncate = False)"
      ],
      "metadata": {
        "id": "gcYKAHKy-NMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: the same error occurs here. It is a less efficient function\n",
        "def find_pairs_supp(rdd_baskets, pairs_list):\n",
        "  iter_rdd = rdd_baskets.toLocalIterator()\n",
        "  pairs_supp = []\n",
        "\n",
        "  for pair in pairs_list:\n",
        "    count_supp = 0\n",
        "    for basket in iter_rdd:\n",
        "      if(set(list(pair))).issubset(set(basket)):\n",
        "        count_supp += 1\n",
        "    pairs_supp.append(pair, count_supp)\n",
        "\n",
        "  return pairs_supp"
      ],
      "metadata": {
        "id": "syi28fLkOiz5"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supp_freq_pairs = find_pairs_supp(rdd_baskets, pairs_list)"
      ],
      "metadata": {
        "id": "YbW4z_WYQQJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the confidence (example of calculated confidence)\n",
        "confidence_I1_I2 = round(I1_I2_support / I1, 2)\n",
        "confidence_I2_I1 = round(I1_I2_support / I2, 2)"
      ],
      "metadata": {
        "id": "gmRCcw90ihzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IhD730ZVvAGc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}